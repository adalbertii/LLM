{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install compress_pickle"
      ],
      "metadata": {
        "id": "Tv2Zeuc8fRes",
        "outputId": "131ca8d2-8f17-400e-d2ff-46d82824084d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting compress_pickle\n",
            "  Downloading compress_pickle-2.1.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: compress_pickle\n",
            "Successfully installed compress_pickle-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip  install deepparse"
      ],
      "metadata": {
        "id": "JjICjK0HfbsC",
        "outputId": "0d9bf80b-3e04-4582-e396-b9f7d8f80f80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepparse\n",
            "  Downloading deepparse-0.9.9-py3-none-any.whl (225 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/225.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/225.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.6/225.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepparse) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepparse) (2.2.1+cu121)\n",
            "Collecting bpemb (from deepparse)\n",
            "  Downloading bpemb-0.3.5-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: gensim>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from deepparse) (4.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from deepparse) (2.31.0)\n",
            "Collecting fasttext-wheel (from deepparse)\n",
            "  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymagnitude-light (from deepparse)\n",
            "  Downloading pymagnitude_light-0.1.147-py3-none-any.whl (35 kB)\n",
            "Collecting poutyne (from deepparse)\n",
            "  Downloading Poutyne-1.17.1-py3-none-any.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.5/213.5 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.10/dist-packages (from deepparse) (2.0.3)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from deepparse) (2.0.7)\n",
            "Requirement already satisfied: cloudpathlib[azure,gs,s3] in /usr/local/lib/python3.10/dist-packages (from deepparse) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->deepparse) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->deepparse) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->deepparse) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.0.0->deepparse) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.0.0->deepparse) (6.4.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from bpemb->deepparse) (0.1.99)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from bpemb->deepparse) (4.66.2)\n",
            "Requirement already satisfied: typing_extensions>4 in /usr/local/lib/python3.10/dist-packages (from cloudpathlib[azure,gs,s3]->deepparse) (4.11.0)\n",
            "Collecting azure-storage-blob>=12 (from cloudpathlib[azure,gs,s3]->deepparse)\n",
            "  Downloading azure_storage_blob-12.19.1-py3-none-any.whl (394 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.5/394.5 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3 (from cloudpathlib[azure,gs,s3]->deepparse)\n",
            "  Downloading boto3-1.34.84-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (from cloudpathlib[azure,gs,s3]->deepparse) (2.8.0)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel->deepparse)\n",
            "  Downloading pybind11-2.12.0-py3-none-any.whl (234 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.0/235.0 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel->deepparse) (67.7.2)\n",
            "Collecting torchmetrics (from poutyne->deepparse)\n",
            "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasteners>=0.14.1 (from pymagnitude-light->deepparse)\n",
            "  Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Collecting lz4>=1.0.0 (from pymagnitude-light->deepparse)\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash>=1.0.1 (from pymagnitude-light->deepparse)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->deepparse) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->deepparse) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->deepparse) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepparse) (3.13.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepparse) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepparse) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepparse) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepparse) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->deepparse)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->deepparse)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->deepparse)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->deepparse)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->deepparse)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->deepparse)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->deepparse)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->deepparse)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->deepparse)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->deepparse)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->deepparse)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepparse) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->deepparse)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Collecting azure-core<2.0.0,>=1.28.0 (from azure-storage-blob>=12->cloudpathlib[azure,gs,s3]->deepparse)\n",
            "  Downloading azure_core-1.30.1-py3-none-any.whl (193 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.4/193.4 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob>=12->cloudpathlib[azure,gs,s3]->deepparse) (42.0.5)\n",
            "Collecting isodate>=0.6.1 (from azure-storage-blob>=12->cloudpathlib[azure,gs,s3]->deepparse)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas==2.0.3->deepparse) (1.16.0)\n",
            "Collecting botocore<1.35.0,>=1.34.84 (from boto3->cloudpathlib[azure,gs,s3]->deepparse)\n",
            "  Downloading botocore-1.34.84-py3-none-any.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->cloudpathlib[azure,gs,s3]->deepparse)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->cloudpathlib[azure,gs,s3]->deepparse)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->cloudpathlib[azure,gs,s3]->deepparse) (2.27.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->cloudpathlib[azure,gs,s3]->deepparse) (2.11.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->cloudpathlib[azure,gs,s3]->deepparse) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->cloudpathlib[azure,gs,s3]->deepparse) (2.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepparse) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepparse) (1.3.0)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics->poutyne->deepparse) (24.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics->poutyne->deepparse)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.1.4->azure-storage-blob>=12->cloudpathlib[azure,gs,s3]->deepparse) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->cloudpathlib[azure,gs,s3]->deepparse) (1.63.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->cloudpathlib[azure,gs,s3]->deepparse) (3.20.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage->cloudpathlib[azure,gs,s3]->deepparse) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage->cloudpathlib[azure,gs,s3]->deepparse) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage->cloudpathlib[azure,gs,s3]->deepparse) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage->cloudpathlib[azure,gs,s3]->deepparse) (1.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12->cloudpathlib[azure,gs,s3]->deepparse) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage->cloudpathlib[azure,gs,s3]->deepparse) (0.6.0)\n",
            "Installing collected packages: xxhash, pybind11, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lz4, lightning-utilities, jmespath, isodate, fasteners, pymagnitude-light, nvidia-cusparse-cu12, nvidia-cudnn-cu12, fasttext-wheel, botocore, azure-core, s3transfer, nvidia-cusolver-cu12, bpemb, azure-storage-blob, boto3, torchmetrics, poutyne, deepparse\n",
            "Successfully installed azure-core-1.30.1 azure-storage-blob-12.19.1 boto3-1.34.84 botocore-1.34.84 bpemb-0.3.5 deepparse-0.9.9 fasteners-0.19 fasttext-wheel-0.9.2 isodate-0.6.1 jmespath-1.0.1 lightning-utilities-0.11.2 lz4-4.3.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 poutyne-1.17.1 pybind11-2.12.0 pymagnitude-light-0.1.147 s3transfer-0.10.1 torchmetrics-1.3.2 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "j5nY4JcnVEu4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import compress_pickle\n",
        "import pickle\n",
        "from deepparse import download_from_public_repository\n",
        "from deepparse.dataset_container import PickleDatasetContainer\n",
        "from deepparse.parser import AddressParser\n",
        "import shutil\n",
        "from poutyne import set_seeds\n",
        "import poutyne\n",
        "import timeit\n",
        "\n",
        "seed = 42\n",
        "set_seeds(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Retrain an Address Parser for Single Country Uses"
      ],
      "metadata": {
        "id": "25k61ikbVEu5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXRz_H4OVEu6"
      },
      "source": [
        "In this example, we will retrain a pre-trained model to maximize its performance for specific countries (e.g. the UK or Canada)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtEyzjnuVEu8"
      },
      "source": [
        "## Retrain a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYXzaDdOVEu8"
      },
      "source": [
        "First, to retrain our supervised model, we need parsed address example, as shown in the following figure. Fortunately, we have access to a public dataset of such parsed examples, the [Structured Multinational Address Dataset](https://github.com/GRAAL-Research/deepparse-address-data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci5HaCo_VEu9"
      },
      "source": [
        "![parsing](https://github.com/GRAAL-Research/deepparse/blob/master/docs/source/_static/img/address_parsing.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrWdeODXVEu9"
      },
      "source": [
        "For our example, we will focus on UK addresses since we want to parse addresses only from the UK. So let's first download the dataset directly from the public repository using Deepparse `download_from_public_repository` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_dJL9fxDVEu-"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"dataset\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download_from_public_repository(\"dataset/data\", \"\", file_extension=\"zip\")"
      ],
      "metadata": {
        "id": "s--0Uu_mgEpN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4uScuG8VEu_"
      },
      "source": [
        "The dataset archive is a zip directory of subdirectories in which each country's data is compressed into an LZMA file (a more aggressive compression algorithm). The dataset public repository offers a [script](https://github.com/GRAAL-Research/deepparse-address-data/blob/main/lzma_decompress.py) to decompress the LZMA compress dataset zip archive. We will use the basic idea of it to decompress the dataset in the next code cell (the script handles CLI parameters)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NoSizABKVEvA"
      },
      "outputs": [],
      "source": [
        "# First, let's decompress the archive\n",
        "archive_root_path = os.path.join(\"dataset\")\n",
        "archive_path = os.path.join(archive_root_path, \"data.zip\")\n",
        "\n",
        "# Unzip the archive\n",
        "shutil.unpack_archive(archive_path, archive_root_path)\n",
        "\n",
        "# Delete the archive\n",
        "os.remove(archive_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jLwJWufmVEvB"
      },
      "outputs": [],
      "source": [
        "# The script functions with minor modification to handle argument\n",
        "# instead or CLI parsed argument\n",
        "\n",
        "\n",
        "# Function to handle the files paths\n",
        "def absolute_file_paths(directory):\n",
        "    \"\"\"\n",
        "    Function to get all the absolute paths of files into a directory.\n",
        "    \"\"\"\n",
        "    for dir_path, _, filenames in os.walk(directory):\n",
        "        for f in filenames:\n",
        "            if f.endswith(\".lzma\"):\n",
        "                yield os.path.abspath(os.path.join(dir_path, f))\n",
        "\n",
        "\n",
        "# Function to LZMA decompress the files_directory into the path_to_save directory\n",
        "def lzma_decompress(files_directory, root_path_to_save) -> None:\n",
        "    \"\"\"\n",
        "    Script to decompress the dataset from LZMA compress files into pickled one.\n",
        "    \"\"\"\n",
        "    paths = absolute_file_paths(files_directory)\n",
        "\n",
        "    for path in paths:\n",
        "        pickled_data = compress_pickle.load(path, compression=\"lzma\")\n",
        "        filename = path.split(os.path.sep)[-1].replace(\".lzma\", \".p\")\n",
        "        file_path = os.path.join(*path.split(os.path.sep)[-4:-1])\n",
        "        path_to_save = os.path.join(root_path_to_save, file_path)\n",
        "        os.makedirs(path_to_save, exist_ok=True)\n",
        "        with open(os.path.join(path_to_save, filename), \"wb\") as file:\n",
        "            pickle.dump(pickled_data, file)\n",
        "        os.remove(path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V1aK4lhSk4s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cGHjjHstVEvC"
      },
      "outputs": [],
      "source": [
        "# Let's decompress the dataset. It takes several minutes to decompress.\n",
        "\n",
        "root_dir = os.path.join(\"dataset\", \"data\")\n",
        "clean_root_dir = os.path.join(root_dir, \"clean_data\")\n",
        "clean_train_directory = os.path.join(clean_root_dir, \"train\")\n",
        "clean_test_directory = os.path.join(clean_root_dir, \"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7JUj9wk2VEvC"
      },
      "outputs": [],
      "source": [
        "# We decompress all the dataset\n",
        "lzma_decompress(root_dir, \"dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdDIIlQfVEvC"
      },
      "source": [
        "Now, let's import our train and test datasets into memory to retrain our parser model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BzwC1WbUVEvD"
      },
      "outputs": [],
      "source": [
        "clean_root_dir = os.path.join(root_dir, \"clean_data\")\n",
        "clean_train_directory = os.path.join(clean_root_dir, \"train\")\n",
        "clean_test_directory = os.path.join(clean_root_dir, \"test\")\n",
        "\n",
        "uk_training_data_path = os.path.join(clean_train_directory, \"gb.p\")\n",
        "uk_test_data_path = os.path.join(clean_test_directory, \"gb.p\")\n",
        "\n",
        "training_container = PickleDatasetContainer(uk_training_data_path)\n",
        "test_container = PickleDatasetContainer(uk_test_data_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14bYZCs3VEvD"
      },
      "source": [
        "We will use the FastText one for our base pre-trained model since it is faster to retrain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcR_Ir_DVEvD",
        "outputId": "8c93fca4-4694-4660-9e2c-728958f88c9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/deepparse/parser/address_parser.py:1094: UserWarning: No CUDA device detected, device will be set to 'CPU'.\n",
            "  warnings.warn(\"No CUDA device detected, device will be set to 'CPU'.\", category=UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/deepparse/network/seq2seq.py:102: UserWarning: No pre-trained model where found in the cache directory /root/.cache/deepparse. Thus, we willautomatically download the pre-trained model.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading the pre-trained weights for the network fasttext.\n",
            "The fastText pretrained word embeddings will be downloaded (6.8 GO), this process will take several minutes.\n",
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.bin.gz\n",
            "(9.03%) [====>                                              ]"
          ]
        }
      ],
      "source": [
        "address_parser = AddressParser(model_type=\"fasttext\", device=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pghrZne-VEvE"
      },
      "source": [
        "But first, let's see what the performance is before retraining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJkD3SjxVEvE"
      },
      "outputs": [],
      "source": [
        "address_parser.test(test_container, batch_size=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "lu1pokS1VEvG",
        "outputId": "56f6cea2-4317-414c-a184-93cee637b664"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[35mEpoch: \u001b[36m1/5 \u001b[35mTrain steps: \u001b[36m2500 \u001b[35mVal steps: \u001b[36m625 \u001b[32m57.82s \u001b[35mloss:\u001b[94m 0.096870\u001b[35m accuracy:\u001b[94m 99.663765\u001b[35m val_loss:\u001b[94m 0.105059\u001b[35m val_accuracy:\u001b[94m 99.660023\u001b[0m\n",
            "Epoch 1: val_loss improved from inf to 0.10506, saving file to ./uk_faster_retrain/checkpoint_epoch_1.ckpt\n",
            "\u001b[35mEpoch: \u001b[36m2/5 \u001b[35mTrain steps: \u001b[36m2500 \u001b[35mVal steps: \u001b[36m625 \u001b[32m58.84s \u001b[35mloss:\u001b[94m 0.092458\u001b[35m accuracy:\u001b[94m 99.677379\u001b[35m val_loss:\u001b[94m 0.103238\u001b[35m val_accuracy:\u001b[94m 99.672032\u001b[0m\n",
            "Epoch 2: val_loss improved from 0.10506 to 0.10324, saving file to ./uk_faster_retrain/checkpoint_epoch_2.ckpt\n",
            "\u001b[35mEpoch: \u001b[36m3/5 \u001b[35mTrain steps: \u001b[36m2500 \u001b[35mVal steps: \u001b[36m625 \u001b[32m58.43s \u001b[35mloss:\u001b[94m 0.090964\u001b[35m accuracy:\u001b[94m 99.683519\u001b[35m val_loss:\u001b[94m 0.103035\u001b[35m val_accuracy:\u001b[94m 99.673781\u001b[0m\n",
            "Epoch 3: val_loss improved from 0.10324 to 0.10304, saving file to ./uk_faster_retrain/checkpoint_epoch_3.ckpt\n",
            "\u001b[35mEpoch: \u001b[36m4/5 \u001b[35mTrain steps: \u001b[36m2500 \u001b[35mVal steps: \u001b[36m625 \u001b[32m58.37s \u001b[35mloss:\u001b[94m 0.089921\u001b[35m accuracy:\u001b[94m 99.685827\u001b[35m val_loss:\u001b[94m 0.103027\u001b[35m val_accuracy:\u001b[94m 99.673781\u001b[0m\n",
            "Epoch 4: val_loss improved from 0.10304 to 0.10303, saving file to ./uk_faster_retrain/checkpoint_epoch_4.ckpt\n",
            "\u001b[35mEpoch: \u001b[36m5/5 \u001b[35mTrain steps: \u001b[36m2500 \u001b[35mVal steps: \u001b[36m625 \u001b[32m58.31s \u001b[35mloss:\u001b[94m 0.090967\u001b[35m accuracy:\u001b[94m 99.684051\u001b[35m val_loss:\u001b[94m 0.103027\u001b[35m val_accuracy:\u001b[94m 99.673781\u001b[0m\n",
            "Epoch 5: val_loss improved from 0.10303 to 0.10303, saving file to ./uk_faster_retrain/checkpoint_epoch_5.ckpt\n",
            "Restoring data from ./uk_faster_retrain/checkpoint_epoch_5.ckpt\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'epoch': 1,\n",
              "  'time': 57.81894668377936,\n",
              "  'loss': 0.09687001281384341,\n",
              "  'accuracy': 99.66376511882362,\n",
              "  'val_loss': 0.1050588409877063,\n",
              "  'val_accuracy': 99.66002270373627},\n",
              " {'epoch': 2,\n",
              "  'time': 58.83542291820049,\n",
              "  'loss': 0.09245755615734887,\n",
              "  'accuracy': 99.67737932159272,\n",
              "  'val_loss': 0.10323802052373769,\n",
              "  'val_accuracy': 99.67203222624765},\n",
              " {'epoch': 3,\n",
              "  'time': 58.427432637661695,\n",
              "  'loss': 0.090964393501016,\n",
              "  'accuracy': 99.68351861406266,\n",
              "  'val_loss': 0.1030354176204891,\n",
              "  'val_accuracy': 99.67378086061039},\n",
              " {'epoch': 4,\n",
              "  'time': 58.374891674146056,\n",
              "  'loss': 0.08992147407911884,\n",
              "  'accuracy': 99.68582667894091,\n",
              "  'val_loss': 0.10302727049831104,\n",
              "  'val_accuracy': 99.67378086061039},\n",
              " {'epoch': 5,\n",
              "  'time': 58.309186859056354,\n",
              "  'loss': 0.09096737359163201,\n",
              "  'accuracy': 99.68405119942516,\n",
              "  'val_loss': 0.10302683650516546,\n",
              "  'val_accuracy': 99.67378086061039}]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "_ = address_parser.retrain(\n",
        "    training_container,\n",
        "    train_ratio=0.8,\n",
        "    epochs=1,\n",
        "    batch_size=32,\n",
        "    num_workers=2,\n",
        "    learning_rate=0.001,\n",
        "    logging_path=\"./uk_retrain\",\n",
        "    name_of_the_retrain_parser=\"UKParser\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpfefM6-VEvH",
        "outputId": "2c40e7a2-6c3c-4725-9370-fd901be755e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running test\n",
            "\u001b[35mTest steps: \u001b[36m57 \u001b[32m1.74s \u001b[35mtest_loss:\u001b[94m 0.120875\u001b[35m test_accuracy:\u001b[94m 99.575062\u001b[0m                                                \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'time': 1.7367468271404505,\n",
              " 'test_loss': 0.12087451704787924,\n",
              " 'test_accuracy': 99.5750624169449}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "address_parser.test(test_container, batch_size=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oUBiFrjVEvH"
      },
      "source": [
        "To further improve performance, we could train for longer, increase the training dataset size (the actual size of 100,000 addresses), or rework the Seq2Seq hidden sizes. See the [retrain interface documentation](https://deepparse.org/parser.html#deepparse.parser.AddressParser.retrain) for all the training parameters."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}